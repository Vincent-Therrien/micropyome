{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from torch import nn, tensor, utils, optim, no_grad, Tensor\n",
    "import torch\n",
    "\n",
    "from micropyome.taxa import regression\n",
    "from micropyome.datasets import normalize\n",
    "\n",
    "DATASET_PATH = \"data/averill_processed/bacteria/\"\n",
    "X = {}\n",
    "Y = {}\n",
    "for level in regression.TAXONOMIC_LEVELS:\n",
    "    x = pd.read_csv(f\"{DATASET_PATH}{level}/15_variables.csv\")\n",
    "    if level == 'fg':\n",
    "        y = pd.read_csv(f\"{DATASET_PATH}{level}/observed.csv\")\n",
    "    else:\n",
    "        y = pd.read_csv(f\"{DATASET_PATH}{level}/y_11groupTaxo.csv\")\n",
    "\n",
    "    x = x.drop(x.columns[0], axis=1)\n",
    "    x = x.drop(\"longitude\", axis=1)\n",
    "    x = normalize(x)\n",
    "    y = y.drop(y.columns[0], axis=1)\n",
    "\n",
    "    X[level] = x\n",
    "    Y[level] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X[\"fg\"].values, Y[\"fg\"].values, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(9, 6),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(6, 9),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(9, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 1 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 2 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 3 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 4 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 5 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 6 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 7 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 8 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 9 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 10 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 11 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 12 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 13 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 14 finished\n",
      "Loss after mini-batch     1: 0.070\n",
      "Epoch 15 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 16 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 17 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 18 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 19 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 20 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 21 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 22 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 23 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 24 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 25 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 26 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 27 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 28 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 29 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 30 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 31 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 32 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 33 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 34 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 35 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 36 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 37 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 38 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 39 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 40 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 41 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 42 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 43 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 44 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 45 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 46 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 47 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 48 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 49 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 50 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 51 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 52 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 53 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 54 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 55 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 56 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 57 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 58 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 59 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 60 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 61 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 62 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 63 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 64 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 65 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 66 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 67 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 68 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 69 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 70 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 71 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 72 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 73 finished\n",
      "Loss after mini-batch     1: 0.069\n",
      "Epoch 74 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 75 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 76 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 77 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 78 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 79 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 80 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 81 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 82 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 83 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 84 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 85 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 86 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 87 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 88 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 89 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 90 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 91 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 92 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 93 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 94 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 95 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 96 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 97 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 98 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 99 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 100 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 101 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 102 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 103 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 104 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 105 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 106 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 107 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 108 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 109 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 110 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 111 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 112 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 113 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 114 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 115 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 116 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 117 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 118 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 119 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 120 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 121 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 122 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 123 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 124 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 125 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 126 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 127 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 128 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 129 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 130 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 131 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 132 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 133 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 134 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 135 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 136 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 137 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 138 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 139 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 140 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 141 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 142 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 143 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 144 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 145 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 146 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 147 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 148 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 149 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 150 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 151 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 152 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 153 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 154 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 155 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 156 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 157 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 158 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 159 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 160 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 161 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 162 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 163 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 164 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 165 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 166 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 167 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 168 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 169 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 170 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 171 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 172 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 173 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 174 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 175 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 176 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 177 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 178 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 179 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 180 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 181 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 182 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 183 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 184 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 185 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 186 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 187 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 188 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 189 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 190 finished\n",
      "Loss after mini-batch     1: 0.068\n",
      "Epoch 191 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 192 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 193 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 194 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 195 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 196 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 197 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 198 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 199 finished\n",
      "Loss after mini-batch     1: 0.067\n",
      "Epoch 200 finished\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "autoencoder = AutoEncoder(len(x_train[0]))\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "trainloader = utils.data.DataLoader(\n",
    "    [(x, y) for x, y in zip(x_train, y_train)], batch_size=16\n",
    ")\n",
    "testloader = utils.data.DataLoader(\n",
    "    [(x, y) for x, y in zip(x_test, y_test)], batch_size=16\n",
    ")\n",
    "for epoch in range(EPOCHS):\n",
    "    current_loss = 0.0\n",
    "    for i, (inputs, _) in enumerate(trainloader, 0):\n",
    "        inputs = inputs.float()\n",
    "        optimizer.zero_grad()\n",
    "        encoded, decoded = autoencoder(inputs)\n",
    "        loss = loss_function(decoded, inputs)  # Auto-encoding\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()\n",
    "        if i%100 == 0:\n",
    "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss))\n",
    "            current_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "\n",
    "print(\"Training has completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4546    0.7928        0.3382\n",
      "0.5561    0.2487        0.3074\n",
      "0.4952    0.0        0.4952\n",
      "0.4949    0.3248        0.1701\n",
      "0.4357    0.5574        0.1217\n",
      "0.5524    0.3769        0.1755\n",
      "0.5156    0.497        0.01859\n",
      "0.5013    0.09        0.4113\n",
      "0.5348    0.05604        0.4787\n",
      "0.4463    0.08617        0.3602\n",
      "0.4573    0.5932        0.1359\n",
      "0.4954    0.6735        0.1781\n",
      "0.4835    0.7968        0.3133\n",
      "0.5129    0.4418        0.07109\n"
     ]
    }
   ],
   "source": [
    "autoencoder.eval()\n",
    "with no_grad():\n",
    "    encoded, outputs = autoencoder(Tensor(x_test))\n",
    "    predicted_labels = outputs.squeeze().tolist()\n",
    "\n",
    "for i, o in zip(predicted_labels[0], x_test[0]):\n",
    "    print(f\"{i:.4}    {o:.4}        {abs(i-o):.4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.07702980850449663\n",
      "R2 Score: -1.1041962995204788\n"
     ]
    }
   ],
   "source": [
    "from torch import no_grad, Tensor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(x_test, predicted_labels)\n",
    "r2 = r2_score(x_test, predicted_labels)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_features):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(9, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(9, n_features),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\stage\\micropyome\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 1 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 2 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 3 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 4 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 5 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 6 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 7 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 8 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 9 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 10 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 11 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 12 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 13 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 14 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 15 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 16 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 17 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 18 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 19 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 20 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 21 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 22 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 23 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 24 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 25 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 26 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 27 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 28 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 29 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 30 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 31 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 32 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 33 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 34 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 35 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 36 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 37 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 38 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 39 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 40 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 41 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 42 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 43 finished\n",
      "Loss after mini-batch     1: 0.052\n",
      "Epoch 44 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 45 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 46 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 47 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 48 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 49 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 50 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 51 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 52 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 53 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 54 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 55 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 56 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 57 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 58 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 59 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 60 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 61 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 62 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 63 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 64 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 65 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 66 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 67 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 68 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 69 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 70 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 71 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 72 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 73 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 74 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 75 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 76 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 77 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 78 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 79 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 80 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 81 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 82 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 83 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 84 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 85 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 86 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 87 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 88 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 89 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 90 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 91 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 92 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 93 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 94 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 95 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 96 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 97 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 98 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 99 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 100 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 101 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 102 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 103 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 104 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 105 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 106 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 107 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 108 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 109 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 110 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 111 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 112 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 113 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 114 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 115 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 116 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 117 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 118 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 119 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 120 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 121 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 122 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 123 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 124 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 125 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 126 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 127 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 128 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 129 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 130 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 131 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 132 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 133 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 134 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 135 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 136 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 137 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 138 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 139 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 140 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 141 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 142 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 143 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 144 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 145 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 146 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 147 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 148 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 149 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 150 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 151 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 152 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 153 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 154 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 155 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 156 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 157 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 158 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 159 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 160 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 161 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 162 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 163 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 164 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 165 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 166 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 167 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 168 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 169 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 170 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 171 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 172 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 173 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 174 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 175 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 176 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 177 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 178 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 179 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 180 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 181 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 182 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 183 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 184 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 185 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 186 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 187 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 188 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 189 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 190 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 191 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 192 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 193 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 194 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 195 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 196 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 197 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 198 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 199 finished\n",
      "Loss after mini-batch     1: 0.051\n",
      "Epoch 200 finished\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "predictor = Predictor(len(encoded[0]), len(y_train[0]))\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(predictor.parameters(), lr=1e-4)\n",
    "\n",
    "trainloader = utils.data.DataLoader(\n",
    "    [(x, y) for x, y in zip(encoded, y_train)], batch_size=16\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    current_loss = 0.0\n",
    "    for i, (inputs, y) in enumerate(trainloader, 0):\n",
    "        inputs, y = inputs.float(), y.float()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = predictor(inputs)\n",
    "        loss = loss_function(prediction, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()\n",
    "        if i%100 == 0:\n",
    "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss))\n",
    "            current_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "\n",
    "print(\"Training has completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06491    0.83        0.7651\n",
      "0.06485    0.003182        0.06167\n",
      "0.08993    0.01156        0.07837\n",
      "0.05885    0.00681        0.05204\n",
      "0.07233    0.003249        0.06908\n",
      "0.05857    0.03732        0.02126\n",
      "0.05981    0.01776        0.04205\n",
      "0.07393    0.01831        0.05563\n",
      "0.06874    0.02216        0.04658\n",
      "0.05719    0.003617        0.05357\n",
      "0.09101    0.0009327        0.09008\n",
      "0.08145    0.02045        0.061\n",
      "0.08368    0.001876        0.08181\n",
      "0.07475    0.02274        0.052\n"
     ]
    }
   ],
   "source": [
    "predictor.eval()\n",
    "with no_grad():\n",
    "    predictions = predictor(Tensor(encoded))\n",
    "    predicted_labels = predictions.squeeze().tolist()\n",
    "\n",
    "for i, o in zip(predicted_labels[0], y_test[0]):\n",
    "    print(f\"{i:.4}    {o:.4}        {abs(i-o):.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.05032434526618302\n",
      "R2 Score: -2600.2380080626176\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predicted_labels)\n",
    "r2 = r2_score(y_test, predicted_labels)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
